\documentclass{report}

\title{\Huge \textsc{Polar Codes and Other Concepts}}
\author{By Himanshu Sharma}

\date{}

\begin{document}
\maketitle

\section{Error Correcting Codes}
Basic idea is to add redundant bits to the original bit stream. \\
Code Rate $R = \displaystyle \frac{k}{n}$ where $n > k$. 

The limit to the number of errors that can be corrected is given by {\it Shannon's Theorem}.
\subsection{Shannon's Theorem}
Given a noisy channel with channel capacity $C$ and information transmitted at a rate $R$, then if $R < C$ there exist codes that allow the probability of error at the receiver to be made arbitrarily small.
\subsection{Channel Capacity}
It is the tight upper bound on the rate at which information can be reliably transmitted over a communication channel. 
\subsection{Shannon-Hartley Theorem}
An application of the channel capacity concept to an additive white Gaussian noise (AWGN) channel with B Hz bandwidth and signal-to-noise ratio S/N is the Shannonâ€“Hartley theorem:  \\

\begin{equation}
C = B \log_{2}\Bigg(1+\frac{S}{N}\Bigg)
\end{equation}

\section{Hamming Code $(7 \textrm{ } 4)$}
It is a linear error-correcting code that encodes four bits of data into seven bits by adding three parity bits. It is a member of a larger family of Hamming codes, but the term Hamming code often refers to this specific code that Richard W. Hamming introduced in 1950.

\end{document}